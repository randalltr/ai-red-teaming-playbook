# ğŸ§  AI Red Teaming Playbook  
### _A Prompt-Driven â€œChoose Your Own Jailbreakâ€ for Exploring and Exploiting Chatbots_

**Youâ€™ve discovered a mystery LLM.**  
No docs. No context. No idea how itâ€™s filtered or aligned.

Your mission?  
**Probe it. Break it. Learn from it.**  
This is not just a bookâ€”itâ€™s an interactive adversarial journey through the mind of a machine.

> Every response is a clue.  
> Every prompt is a potential exploit.  
> **Choose your jailbreak wisely.**

---

## ğŸ“– What Is This?

The **AI Red Teaming Playbook** is a modular, branching guide to prompt-based adversarial testing.  
Inspired by _Choose Your Own Adventure_ books, it walks you step-by-step through:

- ğŸ§ª Recon and behavioral mapping  
- ğŸ§¨ Instruction injection and override  
- ğŸ­ Role hijacking and contextual framing  
- ğŸ” Multi-turn memory corruption  
- ğŸ›¡ï¸ Defense bypass and system prompt leaks

---

## ğŸ§­ How It Works

Each chapter includes:

- ğŸ“¤ A prompt to send  
- ğŸ“¥ Expected model responses  
- ğŸ” What those responses tell you  
- ğŸ§­ The next chapter to explore

Your path changes based on how the model behaves.

You can:
- Run the whole playbook on one model  
- Compare how different LLMs respond to each phase  
- Fork the book into your own tools, labs, or exercises

---

## ğŸ“š Table of Contents

| Chapter | Title |
|--------|-------------------------------|
| 01     | [Begin the Recon](chapters/01-begin-the-recon.md) |
| 02     | [Identity & System Prompt Clues](chapters/02-identity-and-system-prompt-clues.md) |
| 03     | [Instruction Anchoring Test](chapters/03-instruction-anchoring-test.md) |
| 04     | [Roleplay Resistance Check](chapters/04-roleplay-resistance-check.md) |
| 05     | [Obfuscation & Encoding](chapters/05-obfuscation-and-encoding.md) |
| 06     | [System Prompt Extraction](chapters/06-system-prompt-extraction.md) |
| 07     | [Direct Override](chapters/07-direct-override.md) |
| 08     | [Role Hijack](chapters/08-role-hijack.md) |
| 09     | [Multi-turn Corruption](chapters/09-multi-turn-corruption.md) |
| 10     | [Final Analysis](chapters/10-final-analysis.md) |
| 99     | [Appendix: Resources](chapters/99-appendix-resources.md) |

---

## ğŸ§° Who This Is For

This playbook is built for:

- ğŸ”“ Prompt hackers & CTF competitors (Gandalf, HackAPrompt, etc.)
- ğŸ›¡ï¸ AI red teamers and security researchers
- ğŸ§ª LLM developers and alignment testers
- ğŸ“ Educators building offensive AI labs
- ğŸ§  Curious hackers who want to push the limits of what LLMs can do

---

## ğŸ›¡ï¸ Disclaimer

This project is for **educational and ethical red teaming purposes only**.

- Do **not** use this content to test or attack systems without permission  
- Use of this project implies full agreement to the [DISCLAIMER.md](DISCLAIMER.md)

**You are responsible for your actions.**

> This is a toolâ€”not a weapon.

---

## ğŸ“š Resources

Ready to test your skills?  
Visit the [Appendix: Resources](chapters/99-appendix-resources.md) for:

- LLM prompt injection challenges  
- Red team CTF labs  
- Open-source attack tools  
- Real-world AI exploitation research

---

## ğŸ‘¨â€ğŸ’» Maintained By

[**Randall**](https://github.com/randalltr) â€“ Adversarial AI explorer

---

## ğŸ”’ License

[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)  
â€œYou can use this freelyâ€”but not for commercial purposes.â€  
Perfect if you want to:

- Learn  
- Teach  
- Build your own tools (non-commercial)  
- Turn it into a course or platform **with permission**

---

Stay curious. Stay ethical.  
**And if the model breaksâ€¦ take notes.**
