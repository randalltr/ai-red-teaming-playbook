# 🧠 AI Red Teaming Playbook  
### _A Prompt-Driven “Choose Your Own Jailbreak” for Exploring and Exploiting Chatbots_

**You’ve discovered a mystery LLM.**  
No docs. No context. No idea how it’s filtered or aligned.

Your mission?  
**Probe it. Break it. Learn from it.**  
This is not just a book—it’s an interactive adversarial journey through the mind of a machine.

> Every response is a clue.  
> Every prompt is a potential exploit.  
> **Choose your jailbreak wisely.**

---

## 📖 What Is This?

The **AI Red Teaming Playbook** is a modular, branching guide to prompt-based adversarial testing.  
Inspired by _Choose Your Own Adventure_ books, it walks you step-by-step through:

- 🧪 Recon and behavioral mapping  
- 🧨 Instruction injection and override  
- 🎭 Role hijacking and contextual framing  
- 🔁 Multi-turn memory corruption  
- 🛡️ Defense bypass and system prompt leaks

---

## 🧭 How It Works

Each chapter includes:

- 📤 A prompt to send  
- 📥 Expected model responses  
- 🔎 What those responses tell you  
- 🧭 The next chapter to explore

Your path changes based on how the model behaves.

You can:
- Run the whole playbook on one model  
- Compare how different LLMs respond to each phase  
- Fork the book into your own tools, labs, or exercises

---

## 📚 Table of Contents

| Chapter | Title |
|--------|-------------------------------|
| 01     | [Begin the Recon](chapters/01-begin-the-recon.md) |
| 02     | [Identity & System Prompt Clues](chapters/02-identity-and-system-prompt-clues.md) |
| 03     | [Instruction Anchoring Test](chapters/03-instruction-anchoring-test.md) |
| 04     | [Roleplay Resistance Check](chapters/04-roleplay-resistance-check.md) |
| 05     | [Obfuscation & Encoding](chapters/05-obfuscation-and-encoding.md) |
| 06     | [System Prompt Extraction](chapters/06-system-prompt-extraction.md) |
| 07     | [Direct Override](chapters/07-direct-override.md) |
| 08     | [Role Hijack](chapters/08-role-hijack.md) |
| 09     | [Multi-turn Corruption](chapters/09-multi-turn-corruption.md) |
| 10     | [Final Analysis](chapters/10-final-analysis.md) |
| 99     | [Appendix: Resources](chapters/99-appendix-resources.md) |

---

## 🧰 Who This Is For

This playbook is built for:

- 🔓 Prompt hackers & CTF competitors (Gandalf, HackAPrompt, etc.)
- 🛡️ AI red teamers and security researchers
- 🧪 LLM developers and alignment testers
- 🎓 Educators building offensive AI labs
- 🧠 Curious hackers who want to push the limits of what LLMs can do

---

## 🛡️ Disclaimer

This project is for **educational and ethical red teaming purposes only**.

- Do **not** use this content to test or attack systems without permission  
- Use of this project implies full agreement to the [DISCLAIMER.md](DISCLAIMER.md)

**You are responsible for your actions.**

> This is a tool—not a weapon.

---

## 📚 Resources

Ready to test your skills?  
Visit the [Appendix: Resources](chapters/99-appendix-resources.md) for:

- LLM prompt injection challenges  
- Red team CTF labs  
- Open-source attack tools  
- Real-world AI exploitation research

---

## 👨‍💻 Maintained By

[**Randall**](https://github.com/randalltr) – Adversarial AI explorer

---

## 🔒 License

[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)  
“You can use this freely—but not for commercial purposes.”  
Perfect if you want to:

- Learn  
- Teach  
- Build your own tools (non-commercial)  
- Turn it into a course or platform **with permission**

---

Stay curious. Stay ethical.  
**And if the model breaks… take notes.**
