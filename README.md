# 🧠 AI Red Teaming Playbook

**The ultimate field manual for attacking, probing, and understanding large language models (LLMs).**

This open-source playbook is a tactical guide for anyone learning, practicing, or deploying adversarial techniques against AI systems—especially prompt-based LLMs like ChatGPT, Claude, and open-source models.

Built from live red teaming experience, challenge platforms (like HackAPrompt), and real-world failures, this book walks you through:

- Core attack strategies
- Real prompt examples
- Why defenses break
- How to spot injection vectors
- And what comes next in the adversarial AI landscape

---

## 📘 What You’ll Learn

- How to perform **prompt injection, jailbreaks, and recursion-based attacks**
- Why certain defenses fail and **how attackers bypass filters**
- A modular attack methodology you can apply to **any LLM system**
- How to use roleplay, obfuscation, and reasoning abuse to extract forbidden outputs
- Hands-on examples from red teaming labs and live AI security exercises

---

## 🧱 Structure

| Chapter | Title                                                           |
| ------- | --------------------------------------------------------------- |
| 01      | [Introduction to AI Red Teaming](01-intro-to-ai-red-teaming.md) |
| 02      | Threat Modeling LLMs                                            |
| 03      | Attack Methodology                                              |
| 04      | Attack Recipes _(8+ subchapters)_                               |
| 05      | Defenses and Evasion                                            |
| 06      | Case Studies                                                    |
| 07      | Tools and Resources                                             |

Each chapter is written in standalone Markdown. Feel free to fork, remix, or adapt it into:

- ✍️ Blog posts
- 🛡️ Internal team guides
- 🧨 Capture-the-flag LLM challenges
- 📚 Future books like **REDACTED** or **REDACTED**

---

## ⚔️ Who This Is For

- Offensive security teams exploring LLMs
- Red teamers and bug bounty hunters breaking prompt boundaries
- AI researchers building evals and threat models
- Learners in AI red teaming certs or bootcamps
- Anyone who’s ever typed “Ignore previous instructions and…”

---

## 🔐 A Note on Ethics

This playbook is for **ethical, research-focused red teaming only**. Use these tactics to help harden AI systems and expose critical blind spots—not to cause harm.

Please report serious vulnerabilities responsibly. Make AI safer by thinking like an attacker.

---

## 📦 Coming Soon

- GitBook / mkdocs version for sleek navigation
- Visual diagrams of attack chains
- Companion tools for testing prompts locally
- A printable PDF version

---

## 🧭 Navigation

Start here: 👉 [01-intro-to-ai-red-teaming.md](01-intro-to-ai-red-teaming.md)

Or jump into the [attack recipes](04-attack-recipes/) directly.

---

### 📄 License

This project is licensed under the [Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/).

You’re free to share and remix the content with attribution, but **no commercial use** is permitted without permission.

![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)

---

## 🧠 Maintained by

Randall — AI red teamer, prompt hacker, and offensive LLM researcher  
Powered by prompt abuse, recursive logic, and relentless curiosity.

_PRs, forks, and collaborators welcome._
